timm библиотека моделей
предобработка
обученная модель 
Слои, дерево
Нормализация
Частичное применение функции partial
параметры
функция потерь
mse - среднеквадратичная ошибка
Если потери уменьшаются - мы движемся в правильном направлении
Производная
tensor - тензоры
loss.backward()
with torch.no_grad() использование без подсчета градиента
Оптимизация - градиентный спуск
ReLU - выпрямленная линейная функция
нейроны
Глубокое обучение
полуконтролируемое обучение
скорость обучения learning rate - гиперпараметр
Matrix Multiplications - матричное умножение
бинарные категориальные переменные
нормализация данных
фиктивные переменные

___

 **Resume**: 
 Первая часть урока была посвещена теме выбора лучшей модели (классификатора) и как надо с ними экспериментально работать, чтобы выбрать лучшую. 
 Вторая часть урока была об ML (Машинном обучении). От простого к сложному.
 И в третьей части урока была реализации регрессии с датасетом Titanic в Excel, и важность матричного умножения.

 ___

 **Glossary**:
 mae - среднеквадратичная ошибка, используется для подсчета потерь loss.


 `abc = torch.tensor([1.1,1.1,1.1])`
 Чтобы сказать PyTorch посчитать градиент для этих параметров, мы должны вызвать *requires_grad_()*
 `abc.requires_grad_()`

 Когда мы делаем градиентный спуск, мы минимизируем величину, называемую loss
 ```
loss = quad_mae(abc)
loss
```
Теперь просим PyTorch рассчитать градиенты с помощью *backward()*
`loss.backward()`

Теперь градиенты хранятся в атрибуте *grad*
`abc.grad`

Далее мы увеличиваем наши параметры на lr (learning rate), умноженный на градиент, что должно улучшить наши параметры
```
with torch.no_grad():
    abc -= abc.grad*0.01
    loss = quad_mae(abc)
```

Мы использем новые параметры в `with torch.no_grad()`. Это выключает подсчет градиентов любых операций внутри этого мененджера контекста.

И в конце концов мы делаем цикл, чтобы автоматизировать градиентный спуск
```
for i in range(10):
    loss = quad_mae(abc)
    loss.backward()
    with torch.no_grad(): abc -= abc.grad*0.01
```
___

Нейронная сеть апроксимирует любую функцию очень простыми шагами
1. Матричным умножением и дальнейшим сложением вместе
2. функцией ReLU, которая заменяет отрицательные числа на 0